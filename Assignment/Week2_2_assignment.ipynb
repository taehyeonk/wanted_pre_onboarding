{"cells":[{"cell_type":"markdown","metadata":{"id":"592U6lXs3d2t"},"source":["# Week2_2 Assignment\n","\n","## [BASIC](#Basic) \n","- \"네이버 영화 감성 분류\" 데이터를 불러와 `pandas` 라이브러리를 사용해 **전처리** 할 수 있다.\n","- 적은 데이터로도 높은 성능을 내기 위해, pre-trained `BERT` 모델 위에 1개의 hidden layer를 쌓아 **fine-tuning**할 수 있다.\n","\n","## [CHALLENGE](#Challenge)\n","- 토큰화된 학습 데이터를 배치 단위로 갖는 **traindata iterator**를 구현할 수 있다. \n","\n","## [ADVANCED](#Advanced)\n","- **loss와 optimizer 함수**를 사용할 수 있다. \n","- traindata iterator를 for loop 돌며 **fine-tuning** 할 수 있다.\n","- fine-tuning의 2가지 방법론을 비교할 수 있다. \n","  - BERT 파라미터를 **freeze** 한 채 fine-tuning (Vision에서 주로 사용하는 방법론)\n","  - BERT 파라미터를 **unfreeze** 한 채 fine-tuning (NLP에서 주로 사용하는 방법론)\n","\n","\n","### Reference\n","- [huggingface 한국어 오픈소스 모델](https://huggingface.co/models?language=ko&sort=downloads&search=bert)\n","- [transformer BertForSequenceClassification 소스 코드](https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/bert/modeling_bert.py#L1501)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"KSX-wQA1RD1h","executionInfo":{"status":"ok","timestamp":1646565130724,"user_tz":-540,"elapsed":5159,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["import os\n","import sys\n","import pandas as pd\n","import numpy as np \n","import torch\n","import random"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"4Reyt-HvLnJv","executionInfo":{"status":"ok","timestamp":1646565130725,"user_tz":-540,"elapsed":8,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["# seed\n","seed = 7777\n","random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"gUR6vb3L3d2u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646565131212,"user_tz":-540,"elapsed":492,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}},"outputId":"ab29f281-5558-469f-d077-df804ac07751"},"outputs":[{"output_type":"stream","name":"stdout","text":["# available GPUs : 1\n","GPU name : Tesla T4\n","cuda\n"]}],"source":["# device type\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","  print(f\"# available GPUs : {torch.cuda.device_count()}\")\n","  print(f\"GPU name : {torch.cuda.get_device_name()}\")\n","else:\n","  device = torch.device(\"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"c93M8XmjLnJw"},"source":["## Basic"]},{"cell_type":"markdown","metadata":{"id":"0REKl4EvT9G1"},"source":["### 데이터 다운로드 및 DataFrame 형태로 불러오기\n","- 내 구글 드라이브에 데이터를 다운받은 후 코랩에 드라이브를 마운트하면 데이터를 영구적으로 사용할 수 있음.\n","- [네이버영화감성분류](https://github.com/e9t/nsmc)\n","  - trainset: 150,000 \n","  - testset: 50,000 "]},{"cell_type":"code","execution_count":4,"metadata":{"id":"lEWUggR1R9rS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646565190226,"user_tz":-540,"elapsed":56508,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}},"outputId":"6ed839ea-fb87-4c25-b59e-1c604bcf7966"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"rov1s8IxSLqy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646565228377,"user_tz":-540,"elapsed":459,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}},"outputId":"3b3cdc12-8e7c-4340-ad66-88c7896c8ce6"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/원티드 프리온보딩/Assignment\n"]}],"source":["cd \"/content/drive/MyDrive/원티드 프리온보딩/Assignment\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OjPGnbEjVYmj"},"outputs":[],"source":["# 데이터 다운로드\n","!git clone https://github.com/e9t/nsmc.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SueG9v14YbgF"},"outputs":[],"source":["_CUR_DIR = os.path.abspath(os.curdir)\n","print(f\"My current directory : {_CUR_DIR}\")\n","_DATA_DIR = os.path.join(_CUR_DIR, \"nsmc\")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"9J6KQ8dzaHBi","executionInfo":{"status":"ok","timestamp":1646565379445,"user_tz":-540,"elapsed":1668,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["# nsmc/ratings_train.txt를 DataFrame 형태로 불러오기\n","df = pd.read_table('ratings_train.txt')"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"3cUsoBEPahlo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646565382513,"user_tz":-540,"elapsed":438,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}},"outputId":"47e983a2-4b0f-4a67-e1d2-5ffa2f7f2eb6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(150000, 3)"]},"metadata":{},"execution_count":7}],"source":["# 데이터 크기 확인\n","df.shape"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Ic3k9CORaXzM","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1646565384510,"user_tz":-540,"elapsed":6,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}},"outputId":"62b66089-3883-45d0-9bb0-8f949fab2417"},"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-d984ed6f-6454-4b75-817d-f8843c4db0c9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9976970</td>\n","      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3819312</td>\n","      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10265843</td>\n","      <td>너무재밓었다그래서보는것을추천한다</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9045019</td>\n","      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6483659</td>\n","      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d984ed6f-6454-4b75-817d-f8843c4db0c9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d984ed6f-6454-4b75-817d-f8843c4db0c9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d984ed6f-6454-4b75-817d-f8843c4db0c9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["         id                                           document  label\n","0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n","1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n","2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n","3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n","4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"]},"metadata":{},"execution_count":8}],"source":["# 데이터 일부 확인\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"JA1F0tHWLnJz"},"source":["### 데이터 결측치 제거 및 데이터 수 줄이기 \n","- 학습 데이터 수는 150,000개로 매우 많은 양이다. 하지만 우리가 실생활에서 마주할 데이터는 이렇게 많지 않다. 이 때 유용하게 사용되는 것이 **fine-tuning** 학습 방법이다.   \n","- Fine-tuning은 단어의 의미를 이미 충분히 학습한 모델 (여기서는 **BERT**)을 가져와 그 위에 추가적인 Nueral Network 레이어를 쌓은 후 학습하는 방법론이다. 이미 BERT가 단어의 의미를 충분히 학습했기 때문에 **적은 데이터**로 학습해도 우수한 성능을 낼 수 있다는 장점이 있다. \n","- **데이터의 label의 비율이 5:5를 유지하면서** 학습 데이터 수를 150,000개에서 1,000개로 줄이\b는 함수 `label_evenly_balanced_dataset_sampler`를 구현하라.\n","  - 함수 정의 \n","    - 입력 매개변수\n","      - df : DataFrame\n","      - n_sample : df에서 샘플링할 row의 개수 (여기서는 1000개로 정의한다)\n","    - 조건\n","      - label의 비율이 5:5를 유지할 수 있도록 샘플링한다.\n","    - 반환값\n","      - row의 개수가 1000개인 dataframe"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Lh9BSiSeMms7","executionInfo":{"status":"ok","timestamp":1646565471274,"user_tz":-540,"elapsed":507,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["# df에서 결측치 (na 값) 제거\n","\n","df = df.dropna(axis=0)"]},{"cell_type":"code","source":["df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F6lO74eM8z3m","executionInfo":{"status":"ok","timestamp":1646565490458,"user_tz":-540,"elapsed":486,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}},"outputId":"add7adb8-d180-4a3d-dadf-bbc6d0bb4c41"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(149995, 3)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ommF5KH4akCJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646565605977,"user_tz":-540,"elapsed":7,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}},"outputId":"542b98f6-fa52-4d4d-b273-58807ddf06f8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    75170\n","1    74825\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":13}],"source":["# label별 데이터 수 확인\n","# pandas의 value_counts 함수 활용\n","# 0 -> 부정 1 -> 긍정\n","\n","df['label'].value_counts()"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"_ii06wCsc107","executionInfo":{"status":"ok","timestamp":1646565633648,"user_tz":-540,"elapsed":1053,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["# 학습 데이터 샘플 개수 설정\n","\n","n_sample = 1000"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"Hrkhl69Dc-kr","executionInfo":{"status":"ok","timestamp":1646566983135,"user_tz":-540,"elapsed":528,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["# 샘플링 함수 구현\n","# random 모듈에서 제공되는 함수 활용\n","# input: 학습 데이터 샘플 개수\n","# output: 샘플링 데이터\n","\n","\n","def label_evenly_balanced_dataset_sampler(df, sample_size):\n","  \"\"\"\n","  데이터 프레임의을 sample_size만큼 임의 추출해 새로운 데이터 프레임을 생성.\n","  이 때, \"label\"열의 값들이 동일한 비율을 갖도록(5:5) 할 것.\n","  \"\"\"\n","\n","  sample = df[df['label']==1].sample(int(sample_size*0.5), random_state = seed).append(df[df['label']==0].sample(int(sample_size*0.5), random_state = seed))\n","\n","  return sample\n","\n","sample_df = label_evenly_balanced_dataset_sampler(df, n_sample)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"hXLT6tAdaA34","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646566986287,"user_tz":-540,"elapsed":504,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}},"outputId":"056664f4-ed96-4c35-b2c8-6d3dbb64d609"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    500\n","0    500\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":26}],"source":["# 검증\n","\n","sample_df.label.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"cLNUjgawLnJ1"},"source":["### CustomClassifier 클래스 구현\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week2/week2_2_bertclf.png?raw=true\" width=400>\n","\n","- 그림과 같이 사전 학습(pre-trained)된 `BERT` 모델을 불러와 그 위에 **1 hidden layer**와 **binary classifier layer**를 쌓아 fine-tunning 모델을 생성할 것이다.    \n","---\n","- hidden layer 1개와 output layer(binary classifier layer)를 갖는 `CustomClassifier` 클래스를 구현하라.\n","- 클래스 정의\n","  - 생성자 입력 매개변수\n","    - `hidden_size` : BERT의 embedding size\n","    - `n_label` : class(label) 개수\n","  - 생성자에서 생성할 변수\n","    - `bert` : BERT 모델 인스턴스 \n","    - `classifier` : 1 hidden layer + relu +  dropout + classifier layer를 stack한 `nn.Sequential` 모델\n","      - 첫번재 히든 레이어 (첫번째 `nn.Linear`)\n","        - input: BERT의 마지막 layer의 1번재 token ([CLS] 토큰) (shape: `hidden_size`)\n","        - output: (shape: `linear_layer_hidden_size`)\n","      - 아웃풋 레이어 (두번째 `nn.Linear`)\n","        - input: 첫번째 히든 레이어의 아웃풋 (shape: `linear_layer_hidden_size`)\n","        - output: target/label의 개수 (shape:2)\n","  - 메소드\n","    - `forward()`\n","      - BERT output에서 마지막 레이어의 첫번째 토큰 ('[CLS]')의 embedding을 가져와 `self.classifier`에 입력해 아웃풋으로 logits를 출력함.\n","  - 주의 사항\n","    - `CustomClassifier` 클래스는 부모 클래스로 `nn.Module`을 상속 받는다.\n"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"U0WbqVv62Zvy","executionInfo":{"status":"ok","timestamp":1646567117710,"user_tz":-540,"elapsed":1045,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"Im98H4-U1eQQ","executionInfo":{"status":"ok","timestamp":1646568158524,"user_tz":-540,"elapsed":510,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["# classifier 구현\n","class CustomClassifier(nn.Module):\n","\n","  def __init__(self, hidden_size: int, n_label: int):\n","    super(CustomClassifier, self).__init__()\n","\n","    self.bert = BertModel.from_pretrained(\"klue/bert-base\")\n","\n","    dropout_rate = 0.1\n","    linear_layer_hidden_size = 32\n","\n","    self.classifier = nn.Sequential(\n","        nn.Linear(in_features=hidden_size, out_features=linear_layer_hidden_size, bias=True),\n","        nn.ReLU(inplace=True),\n","        nn.Dropout(dropout_rate),\n","        nn.Linear(in_features=linear_layer_hidden_size, out_features=n_label, bias=True)\n","    ) # torch.nn에서 제공되는 Sequential, Linear, ReLU, Dropout 함수 활용\n","\n","\n","  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n","\n","    outputs = self.bert(\n","        input_ids,\n","        attention_mask=attention_mask,\n","        token_type_ids=token_type_ids,\n","    )\n","\n","    # BERT 모델의 마지막 레이어의 첫번재 토큰을 인덱싱\n","    cls_token_last_hidden_states = outputs['pooler_output'] # 마지막 layer의 첫 번째 토큰 (\"[CLS]\") 벡터를 가져오기, shape = (1, hidden_size)\n","\n","    logits = self.classifier(cls_token_last_hidden_states)\n","\n","    return logits"]},{"cell_type":"markdown","metadata":{"id":"9x7PU1t1LnJ1"},"source":["## Challenge"]},{"cell_type":"markdown","metadata":{"id":"YXesCG5TLnJ1"},"source":["### 학습 데이터를 배치 단위로 저장하는 이터레이터 함수 `data_iterator` 구현\n","- 데이터 프레임을 입력 받아 text를 \b토큰 id로 변환하고 label은 텐서로 변환해 배치만큼 잘라 (input, \btarget) 튜플 형태의 이터레이터를 생성하는 `data_iterator` 함수를 구현하라.\n","- 함수 정의 \n","  - 입력 매개변수\n","    - `input_column` : text 데이터 column 명\n","    - `target_column` : label 데이터 column 명\n","    -  `batch_size` : 배치 사이즈\n","  - 조건\n","    - 함수는 다음을 수행해야 함 \n","      - 데이터 프레임 랜덤 셔플링\n","      - `tokenizer_bert`로 text를 token_id로 변환 + 텐서화 \n","      - target(label)을 텐서화\n","  - 반환값 \n","    - (input, target) 튜플 형태의 이터레이터를 반환"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"q-tJERGI4Fzk","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646568166498,"user_tz":-540,"elapsed":2875,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}},"outputId":"66737372-b9d7-4f83-b356-a4d8fe65c3d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"JlcYCOyW3d2t","executionInfo":{"status":"ok","timestamp":1646568166976,"user_tz":-540,"elapsed":2,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["from transformers import BertTokenizer, BertModel"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"C_U_c-Mf3d2t","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["855ad399e4e646768e7251588869ee02","3195647c303649a9bc06ba3a6519f89a","58a7dea4c108451eaee501942f4dc30d","4098ba8d52d443259ca7f742bf7d428a","9bf117a6381d47e1bd3dca42548653ab","da7500f18abf47d3b924f765c3bdb888","a4b144e6f7ef451f9ddf36778023372a","170cc00f8a924b91b759ce8d3fe25d6f","60baad0928844231b73d968e960f56a7","7d8308d51b1b4cfc9f4707193ec844e3","9de4ce0f4a7f458891f37cc98dc23a67","b16d00efcb78461784069d19920345b2","7240fa6eb07a45f2a108901d8f129fd2","38710381ee5543f59a2e9286eddb6583","c9ed51ac1f9f4e40a7b351a1acc12cf9","f866540cd08042bb9d298b268d9e5ca3","90e7525805aa46c496ccae0c8d0bc7be","ed494140420344d58e2fe792c5bcef02","5fcd9e7102ec4c48b14bff8365d38f00","a44202268a964b7bbffa4127355a612a","011d3655f3b94b7aa6c3a3f2587f8b2b","48f7a00afc7a44d4a5091b4802f29279","f14d94eec0034f71ab45e4b120501c04","e3714285862a49c3ac86b8045e87ae7f","98b86acbe2ef410b82e59b6534a90a22","ddb9646a1efc4e1c87e6a958a8cc2c75","1d12db13ee1046128eb0d803cbad36f8","1fc27708b6da4cd89a244f6f152e1701","adc5556b263249a88e50b07f814b3927","cc5dfeeb712a4b8dabef8cdcfaf04f04","2d2000a6fd1440ad9f3ce909a57f08a1","38b4c79ab7834313942e7f2414b059c0","b95ea56a113c4e9e91d036b1b3d2a6a5"]},"executionInfo":{"status":"ok","timestamp":1646568181370,"user_tz":-540,"elapsed":9029,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}},"outputId":"ca6e3fc3-e27b-45a9-c89b-34e074a01245"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"855ad399e4e646768e7251588869ee02","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/243k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b16d00efcb78461784069d19920345b2","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f14d94eec0034f71ab45e4b120501c04","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/289 [00:00<?, ?B/s]"]},"metadata":{}}],"source":["tokenizer_bert = BertTokenizer.from_pretrained(\"klue/bert-base\") # lower-cased version"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"p2VnIY-ALnJ2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646568181370,"user_tz":-540,"elapsed":7,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}},"outputId":"042f2886-0606-44c6-ee5d-c2827854f3db"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original Sentence: 스토리는 이상하지만, 멋드러진 액션장면.\n","\n","Tokenized Sentence: {'input_ids': tensor([[    2,  6354,  2259,  3658,  2205,  3683,    16,  1063,  2343,  6814,\n","          8765, 14014,    18,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"]}],"source":["# 토크나이징 예시 (1개의 문장)\n","\n","# 1. string type의 문장을 가져옴\n","ex_sent = sample_df.document.iloc[0]\n","print(f\"Original Sentence: {ex_sent}\\n\")\n","\n","# 2. 문장을 토크나이즈 함. 이 때, 특수 토큰 (\"[CLS]\", \"[SPE]\")을 자동으로 추가하고 pytorch의 tensor형태로 변환해 반환함\n","tensor_sent = tokenizer_bert(\n","    ex_sent,\n","    add_special_tokens=True, # 문장의 앞에 문장 시작을 알리는 \"[CLS]\"토큰, 문장의 끝에 문장 끝을 알리는 \"[SPE]\"토큰을 자동으로 추가\n","    return_tensors='pt' # pytorch tensor로 반환할 것\n",")\n","print(f\"Tokenized Sentence: {tensor_sent}\")"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"rtXP_wRFLnJ2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646568181370,"user_tz":-540,"elapsed":5,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}},"outputId":"e3bd9c91-a398-4eb5-bb3c-13144ec12ab4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original Sentence 1: 스토리는 이상하지만, 멋드러진 액션장면.\n","Original Sentence 2: 감동적이내요 재밋개 봣슴니다 ㅋ 중간에 은근히 사내 남녀차별을 꼬집고잇는것도 특이허내요\n","\n","Tokenized Sentence list: {'input_ids': tensor([[    2,  6354,  2259,  3658,  2205,  3683,    16,  1063,  2343,  6814,\n","          8765, 14014,    18,     3,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0],\n","        [    2,  5869,  2125,  2052,  2369,  2182,  1528,  2997,  2019,     1,\n","           191,  5141,  2170, 14638,  5696,  5969, 25994,  2069, 10973,  2088,\n","          2636,  2259,  2728,  2119,  8329,  2509,  2369,  2182,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1]])}\n"]}],"source":["# 토크나이징 예시 (2개의 문장)\n","\n","# 1. 2개의 문장을 가진 list 생성\n","ex_sent_list = list(sample_df.document.iloc[:2].values)\n","for i, sent in enumerate(ex_sent_list):\n","    print(f\"Original Sentence {i+1}: {sent}\")\n","\n","# 2. 문장 리스트를 토크나이즈 함. 이 때, 리스트 내 문장들의 토큰 길이가 동일할 수 있도록 가장 긴 문장을 기준으로 부족한 위치에 \"[PAD]\" 토큰을 추가\n","tensor_sent_list = tokenizer_bert(\n","    ex_sent_list,\n","    add_special_tokens=True,\n","    return_tensors='pt',\n","    padding=\"longest\" # 가장 긴 문장을 기준으로 token개수를 맞춤. 모자란 토큰 위치는 \"[PAD]\" 토큰을 추가\n",")\n","\n","print(f\"\\nTokenized Sentence list: {tensor_sent_list}\")\n","\n","# 토크나이즈 된 두 문장의 길이가 동일함을 검증\n","assert tensor_sent_list['input_ids'][0].shape == tensor_sent_list['input_ids'][1].shape "]},{"cell_type":"code","execution_count":53,"metadata":{"id":"tR22xs-xf1QH","executionInfo":{"status":"ok","timestamp":1646569740359,"user_tz":-540,"elapsed":1082,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["def data_iterator(df, input_column, target_column, batch_size):\n","  \"\"\"\n","  데이터 프레임을 셔플한 후 \n","  데이터 프레임의 input_column을 batch_size만큼 잘라 토크나이즈 + 텐서화하고, target_column을 batch_size만큼 잘라 텐서화 하여\n","  (input, output) 튜플 형태의 이터레이터를 생성\n","  \"\"\"\n","\n","  global tokenizer_bert\n","\n","  # 1. 데이터 프레임 셔플\n","  #    pandas의 sample 함수 사용\n","  df = df.sample(frac=1).reset_index(drop=True)\n","\n","  # 2. 이터레이터 생성\n","  for idx in range(0, df.shape[0], batch_size):\n","    batch_df = df.loc[idx:idx+batch_size] # batch_size만큼 데이터 추출\n","    \n","    tensorized_input = tokenizer_bert(batch_df[input_column].values, return_tensors='pt') # df의 text를 토크나이징 + token id로 변환 + 텐서화 (df의 input_column 사용)\n","    \n","    tensorized_target = torch.tensor(batch_df[target_column].values) # target(label)을 텐서화 (df의 target_column 사용)\n","    \n","    yield tensorized_input, tensorized_target # 튜플 형태로 yield"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"zTlAV0hqILmc","executionInfo":{"status":"ok","timestamp":1646569741335,"user_tz":-540,"elapsed":4,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["batch_size=32\n","train_iterator = data_iterator(sample_df, 'document', 'label', batch_size)"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"P9VNAMchf1QI","colab":{"base_uri":"https://localhost:8080/","height":382},"executionInfo":{"status":"error","timestamp":1646569743535,"user_tz":-540,"elapsed":5,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}},"outputId":"b0a1e66c-cbfa-4fe6-c2ca-be6c5e7fa7a7"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-55-752a1149bb25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-53-03f7a8e3c213>\u001b[0m in \u001b[0;36mdata_iterator\u001b[0;34m(df, input_column, target_column, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mbatch_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# batch_size만큼 데이터 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtensorized_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# df의 text를 토크나이징 + token id로 변환 + 텐서화 (df의 input_column 사용)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtensorized_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# target(label)을 텐서화 (df의 target_column 사용)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_valid_text_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2437\u001b[0m             raise ValueError(\n\u001b[0;32m-> 2438\u001b[0;31m                 \u001b[0;34m\"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2439\u001b[0m                 \u001b[0;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2440\u001b[0m             )\n","\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."]}],"source":["next(train_iterator)"]},{"cell_type":"markdown","metadata":{"id":"Cqnp2Q6ZLnJ2"},"source":["## Advanced"]},{"cell_type":"markdown","metadata":{"id":"cQVTqAUxLnJ2"},"source":["### `data_iterator` 함수로 생성한 이터레이터를 for loop 돌면서 배치 단위의 데이터를 모델에 학습하는 `train()` 함수 구현\n","- 함수 정의\n","  - 입력 매개변수\n","    - `model` : BERT + 1 hidden layer classifier 모델\n","    - `data_iterator` : train data iterator\n","- Reference\n","  - [Loss: CrossEntropyLoss official document](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n","  - [Optimizer: AdamW official document](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-sE7xjYcRD1p"},"outputs":[],"source":["from torch.optim import AdamW\n","from torch.nn import CrossEntropyLoss\n","from numpy.core.fromnumeric import nonzero"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Er1qKtsf1QJ"},"outputs":[],"source":["# 모델 클래스 정의\n","model = CustomClassifier(hidden_size=768, n_label=2)\n","\n","batch_size = 32\n","\n","# 데이터 이터레이터 정의 \n","train_iterator = None\n","\n","# 로스 및 옵티마이저\n","loss_fct = CrossEntropyLoss()\n","optimizer = AdamW(\n","    model.parameters(),\n","    lr=2e-5,\n","    eps=1e-8\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZvY5rxDKHQAp"},"outputs":[],"source":["def train(model, data_iterator):\n","\n","  global loss_fct # 위에서 정의한 loss 함수\n","\n","  # 배치 단위 평균 loss와 총 평균 loss 계산하기위해 변수 생성\n","  total_loss, batch_loss, batch_count = 0,0,0\n","  \n","  # model을 train 모드로 설정 & device 할당\n","  model.train()\n","  model.to(device)\n","  \n","  # data iterator를 돌면서 하나씩 학습\n","  for step, batch in enumerate(data_iterator):\n","    batch_count+=1\n","    \n","    # tensor 연산 전, 각 tensor에 device 할당\n","    batch = tuple(item.to(device) for item in batch)\n","    \n","    batch_input, batch_label = batch\n","    \n","    # batch마다 모델이 갖고 있는 기존 gradient를 초기화\n","    model.zero_grad()\n","    \n","    # forward\n","    logits = None\n","    \n","    # loss\n","    loss = loss_fct(None, None)\n","    batch_loss += loss.item()\n","    total_loss += loss.item()\n","    \n","    # backward -> 파라미터의 미분(gradient)를 자동으로 계산\n","    loss.backward()\n","    \n","    # optimizer 업데이트\n","    optimizer.step()\n","      \n","    # 배치 10개씩 처리할 때마다 평균 loss를 출력\n","    if (step % 10 == 0 and step != 0):\n","      print(f\"Step : {step}, Avg Loss : {batch_loss / batch_count:.4f}\")\n","      \n","      # 변수 초기화 \n","      batch_loss, batch_count = 0,0\n","  \n","  print(f\"Mean Loss : {total_loss/(step+1):.4f}\")\n","  print(\"Train Finished\")"]},{"cell_type":"markdown","metadata":{"id":"OEYo8z9RLnJ2"},"source":["### 지금까지 구현한 함수와 클래스를 모두 불러와 `train()` 함수를 실행하자\n","- fine-tuning 모델 클래스 (`CustomClassifier`)\n","    - hidden_size = 768\n","    - n_label = 2\n","- 데이터 이터레이터 함수 (`data_iterator`)\n","    - batch_size = 32\n","- loss \n","    - `CrossEntropyLoss()`\n","- optimizer\n","    - optimizer는 loss(오차)를 상쇄하기 위해 파라미터를 업데이트 하는 과정\n","    - `optimizer.step()` 시 파라미터가 업데이트 됨 \n","    - lr = 2e-5\n","- Reference\n","  - [Optimizer 종류 설명 한국어 블로그 ](https://ganghee-lee.tistory.com/24)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCODIxCfFEDP"},"outputs":[],"source":["# 학습 시작\n","train(model, train_iterator)"]},{"cell_type":"markdown","metadata":{"id":"37UbAkh7LnJ3"},"source":["## fine-tuning 2가지 방법론 비교\n","- pre-trained BERT 모델 파라미터를 **freeze**한 채 학습하라\n","    - BERT의 파라미터의 `requires_grad` 값을 `False`로 바꾸면, 학습 시 BERT의 파라미터는 미분이 계산되지도, 업데이트 되지도 않는다. \n","    - 이렇게 특정 모델의 파라미터가 업데이트 하지 못하도록 설정하는 것을 **freeze**라고 한다. \n","    - BERT 파라미터를 freeze시킨 채 학습을 진행해보자. 이럴 경우, 우리가 직접 쌓은 fine-tuning layer의 파라미터만 업데이트 된다. \n","- **unfreeze**와 **freeze** 모델의 성능을 비교해 보자. 어떤 방식이 더 우수한가?\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ld260K3YLnJ3"},"outputs":[],"source":["class CustomClassifierFreezed(nn.Module):\n","\n","  def __init__(self, hidden_size: int, n_label: int):\n","    super(CustomClassifierFreezed, self).__init__()\n","\n","    self.bert = BertModel.from_pretrained(\"klue/bert-base\")\n","    # freeze BERT parameter\n","    # BERT의 파라미터는 고정값으로 두고 BERT 위에 씌운 linear layer의 파라미터만 학습하려고 한다. \n","    # 이 경우, BERT의 파라미터의 'requires_grad' 값을 False로 변경해줘야 학습 시 해당 파라미터의 미분값이 계산되지 않는다.\n","    for param in self.bert.parameters():\n","        None\n","\n","    dropout_rate = 0.1\n","    linear_layer_hidden_size = 32\n","\n","    self.classifier = nn.Sequential(None)\n","\n","      \n","  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n","\n","    outputs = self.bert(\n","        input_ids,\n","        attention_mask=attention_mask,\n","        token_type_ids=token_type_ids,\n","    )\n","    \n","    # BERT 모델의 마지막 레이어의 첫번재 토큰을 인덱싱\n","    cls_token_last_hidden_states = \bout['pooler_output'] # 마지막 layer의 첫 번째 토큰 (\"[CLS]\") 벡터를 가져오기, shape = (1, hidden_size)\n","    logits = self.classifier(cls_token_last_hidden_states)\n","\n","    return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ClEEHB6F6LW"},"outputs":[],"source":["# freeze 모델\n","# model을 제외한 설정값은 \b위에서 실행한 unfreeze 모델과 동일\n","model = None\n","\n","# 데이터 이터레이터\n","batch_size = None \n","train_iterator = None\n","\n","# 로스 및 옵티마이저\n","loss_fct = None\n","optimizer = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hqcPcWZeLnJ3"},"outputs":[],"source":["# 학습 시작\n","train(model, train_iterator)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Week2_2_assignment.ipynb","provenance":[{"file_id":"1h5BjI1snBr9MHQfVUp8MBbnlOWWAJ2Q3","timestamp":1646552088557}]},"kernelspec":{"display_name":"torch","language":"python","name":"torch"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"widgets":{"application/vnd.jupyter.widget-state+json":{"855ad399e4e646768e7251588869ee02":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3195647c303649a9bc06ba3a6519f89a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_58a7dea4c108451eaee501942f4dc30d","IPY_MODEL_4098ba8d52d443259ca7f742bf7d428a","IPY_MODEL_9bf117a6381d47e1bd3dca42548653ab"]}},"3195647c303649a9bc06ba3a6519f89a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"58a7dea4c108451eaee501942f4dc30d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_da7500f18abf47d3b924f765c3bdb888","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a4b144e6f7ef451f9ddf36778023372a"}},"4098ba8d52d443259ca7f742bf7d428a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_170cc00f8a924b91b759ce8d3fe25d6f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":248477,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":248477,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_60baad0928844231b73d968e960f56a7"}},"9bf117a6381d47e1bd3dca42548653ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7d8308d51b1b4cfc9f4707193ec844e3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 243k/243k [00:00&lt;00:00, 274kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9de4ce0f4a7f458891f37cc98dc23a67"}},"da7500f18abf47d3b924f765c3bdb888":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a4b144e6f7ef451f9ddf36778023372a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"170cc00f8a924b91b759ce8d3fe25d6f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"60baad0928844231b73d968e960f56a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7d8308d51b1b4cfc9f4707193ec844e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9de4ce0f4a7f458891f37cc98dc23a67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b16d00efcb78461784069d19920345b2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7240fa6eb07a45f2a108901d8f129fd2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_38710381ee5543f59a2e9286eddb6583","IPY_MODEL_c9ed51ac1f9f4e40a7b351a1acc12cf9","IPY_MODEL_f866540cd08042bb9d298b268d9e5ca3"]}},"7240fa6eb07a45f2a108901d8f129fd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"38710381ee5543f59a2e9286eddb6583":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_90e7525805aa46c496ccae0c8d0bc7be","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ed494140420344d58e2fe792c5bcef02"}},"c9ed51ac1f9f4e40a7b351a1acc12cf9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5fcd9e7102ec4c48b14bff8365d38f00","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":125,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":125,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a44202268a964b7bbffa4127355a612a"}},"f866540cd08042bb9d298b268d9e5ca3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_011d3655f3b94b7aa6c3a3f2587f8b2b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 125/125 [00:00&lt;00:00, 1.03kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_48f7a00afc7a44d4a5091b4802f29279"}},"90e7525805aa46c496ccae0c8d0bc7be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ed494140420344d58e2fe792c5bcef02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5fcd9e7102ec4c48b14bff8365d38f00":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a44202268a964b7bbffa4127355a612a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"011d3655f3b94b7aa6c3a3f2587f8b2b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"48f7a00afc7a44d4a5091b4802f29279":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f14d94eec0034f71ab45e4b120501c04":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e3714285862a49c3ac86b8045e87ae7f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_98b86acbe2ef410b82e59b6534a90a22","IPY_MODEL_ddb9646a1efc4e1c87e6a958a8cc2c75","IPY_MODEL_1d12db13ee1046128eb0d803cbad36f8"]}},"e3714285862a49c3ac86b8045e87ae7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"98b86acbe2ef410b82e59b6534a90a22":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1fc27708b6da4cd89a244f6f152e1701","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_adc5556b263249a88e50b07f814b3927"}},"ddb9646a1efc4e1c87e6a958a8cc2c75":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cc5dfeeb712a4b8dabef8cdcfaf04f04","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":289,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":289,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2d2000a6fd1440ad9f3ce909a57f08a1"}},"1d12db13ee1046128eb0d803cbad36f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_38b4c79ab7834313942e7f2414b059c0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 289/289 [00:00&lt;00:00, 2.07kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b95ea56a113c4e9e91d036b1b3d2a6a5"}},"1fc27708b6da4cd89a244f6f152e1701":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"adc5556b263249a88e50b07f814b3927":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cc5dfeeb712a4b8dabef8cdcfaf04f04":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2d2000a6fd1440ad9f3ce909a57f08a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"38b4c79ab7834313942e7f2414b059c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b95ea56a113c4e9e91d036b1b3d2a6a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}