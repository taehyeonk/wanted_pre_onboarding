{"cells":[{"cell_type":"markdown","metadata":{"id":"592U6lXs3d2t"},"source":["# Week3_4 Assignment\n","\n","## [BASIC](#Basic) \n","- Encoder & Decoder Layer 코드를 직접 필사하고 각 함수에 주석을 달 수 있다. \n","\n","## [CHALLENGE](#Challenge)\n","- 텐서의 크기(shape)를 계산할 수 있다. \n","\n","## [ADVANCED](#Advanced)\n","- 완성된 transformer 모델의 모든 학습 가능한 파라미터 이름과 크기(shape)를 출력할 수 있다.\n","\n","### Informs\n","이번 과제에서는 \"[Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\"의 코드를 필사해본다.   \n","\"Annotated Transformer\"는 \"Attention is all you need\" 논문에서 제안한 transformer 모델을 pytorch 라이브러리로 직접 구현한다.   \n","코드 필사를 통해 다음을 배울 수 있다.    \n","- Encoder, Decoder 구조\n","- Attention Mechanism\n","- \"residual connection\", \"layer normalization\" 등의 구조 "]},{"cell_type":"markdown","metadata":{"id":"GoebvnNZ99r-"},"source":["코드 필사를 시작하기 앞서, transformer 모델의 최종 구조를 살펴보자.    \n","\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_full.png?raw=true\" width=\"500\" align=\"center\"/>\n","\n","최종 모델은 `EncoderDecoder()` 클래스에 여러 인스턴스를 생성자의 입력 파라미터로 넣어 생성한다.    \n","앞으로 우리는 `EncoderDecoder()` 클래스와 같은 여러 클래스들을 구현하고 연결할 것이다. 따라서 대략적인 클래스간의 관계를 살펴보고 이해한다면 보다 큰 그림을 가지고 코드 필사를 할 수 있을 것이다. "]},{"cell_type":"markdown","metadata":{"id":"DB6cNaXP99sB"},"source":["Transformer 모델은 크게 4가지 클래스로 구현된다.    \n","- Frame\n","    - frame 역할을 하는 `EncoderDecoder` 클래스\n","- Input Embedding & Encoding\n","    - 입력값을 벡터화하는 `Embeddings`, `PositionalEncoding`\n","- Encoder & Decoder\n","    - 각 6개 layer를 갖고 있는 `Encoder`, `Decoder`\n","    - layer 1층을 구현한 `EncoderLayer`, `DecoderLayer`\n","- Sublayer\n","    - `EncoderLayer`, `DecoderLayer` 내부에서 사용되는 Sublayer 클래스인 `MultiHeadAttiontion`, `PositionwiseFeedForward`\n","    - Sublayer 클래스들을 연결하는 `SublayerConnection`\n","    \n","아래 좌측 도식에서 각 클래스의 색상은 아래 우측 도식(transformer 구조)의 색상과 맵핑되어 있다.    \n","각 클래스의 역할과 클래스 간 연결 관계를 생각하면서 transformer를 코드로 구현해보자.   \n","\n","\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_map.png?raw=true\" width=\"400\" height=\"400\" align=\"left\"/>\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_transformer.png?raw=true\" width=\"300\" height=\"400\" align=\"right\"/>\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"qaadVYo799sE","executionInfo":{"status":"ok","timestamp":1646904229196,"user_tz":-540,"elapsed":953,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["import os\n","import sys\n","import pandas as pd\n","import numpy as np \n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","import math, copy, time\n","import random"]},{"cell_type":"markdown","metadata":{"id":"1OEO0al299sJ"},"source":["## Basic"]},{"cell_type":"markdown","metadata":{"id":"OKKyKfqB99sL"},"source":["### Frame\n","- `EncoderDecoder`\n","\n","아래 도식은 `EncoderDecoder` 클래스의 `forward()`, `encode()`, `decode()` 메소드를 도식화 한 것이다.    \n"," \n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_encoderdecoder.png?raw=true\" width=500>\n","\n","\n","- `Generator`"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"MECCTGpt99sP","executionInfo":{"status":"ok","timestamp":1646904230751,"user_tz":-540,"elapsed":2,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["class EncoderDecoder(nn.Module):\n","    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n","        super(EncoderDecoder, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_embed = src_embed\n","        self.tgt_embed = tgt_embed\n","        self.generator = generator\n","    \n","    \n","    def forward(self, src, tgt, src_mask, tgt_mask):\n","        encoder_hidden, encoder_final = self.encode(src, src_mask)\n","        return self.decode(encoder_hidden, encoder_final, src_mask, tgt, tgt_mask)\n","    \n","    \n","    def encode(self, src, src_mask):\n","        return self.encoder(self.src_embed(src), src_mask)\n","    \n","    \n","    def decode(self, memory, src_mask, tgt, tgt_mask):\n","        return self.decoder(self.trg_embed(tgt), memory, src_mask, tgt_mask)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Py2wcYPX99sT","executionInfo":{"status":"ok","timestamp":1646904231144,"user_tz":-540,"elapsed":1,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["class Generator(nn.Module):\n","    \n","    def __init__(self, d_model, vocab):\n","        super(Generator, self).__init__()\n","        self.proj = nn.Linear(d_model, vocab)\n","    \n","    \n","    def forward(self, x):\n","        return F.log_softmax(self.proj(x), dim=-1)"]},{"cell_type":"markdown","metadata":{"id":"xI-5SRHD99sX"},"source":["### Encoder\n","- `Encoder`\n","- `EncoderLayer`\n","- `SublayerConnection`\n","- Reference\n","    - Layer Normalization\n","        - [한국어 설명](https://yonghyuc.wordpress.com/2020/03/04/batch-norm-vs-layer-norm/)\n","        - [torch official docs](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)\n","    - Residual Connection\n","        - [한국어 설명](https://itrepo.tistory.com/36)\n","    - pytorch ModuleList\n","        - [torch official docs](https://pytorch.org/docs/1.9.1/generated/torch.nn.ModuleList.html)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"DjIjUBjN99sc","executionInfo":{"status":"ok","timestamp":1646904231793,"user_tz":-540,"elapsed":2,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["def clones(module, N):\n","    # Produce N identical layers\n","    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"wgglBAyM99se","executionInfo":{"status":"ok","timestamp":1646904232200,"user_tz":-540,"elapsed":1,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, layer, N):\n","        super(Encoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","        \n","    def forward(self, x, mask):\n","        # Pass the input (and mask) through each layer in turn\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"AvGP8Gsd99sg","executionInfo":{"status":"ok","timestamp":1646904232201,"user_tz":-540,"elapsed":2,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["class LayerNorm(nn.Module):\n","    \n","    def __init__(self, features, eps=1e-6):\n","        super(LayerNorm, self).__init__()\n","        self.a_2 = nn.Parameter(torch.ones(features))\n","        self.b_2 = nn.Parameter(torch.zeros(features))\n","        self.eps = eps\n","    \n","    \n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"525_O3YE99si","executionInfo":{"status":"ok","timestamp":1646904232520,"user_tz":-540,"elapsed":2,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["class SublayerConnection(nn.Module):\n","    \n","    def __init__(self, size, dropout):\n","        super(SublayerConnection, self).__init__()\n","        self.norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","    \n","    def forward(self, x, sublayer):\n","        return x + self.dropout(sublayer(self.norm(x)))"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"LlGCPEVp99sk","executionInfo":{"status":"ok","timestamp":1646904232521,"user_tz":-540,"elapsed":2,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    \n","    def __init__(self, size, self_attn, feed_forward, dropout):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = self_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n","        self.size = size\n","        \n","    def forward(self, x, mask):\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n","        return self.sublayer[1](x, self.feed_forward)"]},{"cell_type":"markdown","metadata":{"id":"OOiYmYWc99sm"},"source":["### Decoder\n","- `Decoder`\n","- `DecoderLayer`"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Ik47frFO99so","executionInfo":{"status":"ok","timestamp":1646904232875,"user_tz":-540,"elapsed":6,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["class Decoder(nn.Module):\n","    \n","    def __init__(self, layer, N):\n","        super(Decoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","    \n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        for layer in self.layers:\n","            x = layer(x, memory, src_mask, tgt_mask)\n","        return self.norm(x)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"ElsG9P7M99sq","executionInfo":{"status":"ok","timestamp":1646904233129,"user_tz":-540,"elapsed":3,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","\n","    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.size = size\n","        self.self_attn = self_attn\n","        self.src_attn = src_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n","    \n","    \n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        m = memory\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n","        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n","        return self.sublayer[2](x, self.feed_forward)"]},{"cell_type":"markdown","metadata":{"id":"vhPP8LVw99sr"},"source":["### Sublayer\n","- `attention` 함수\n","\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_attention.png?raw=true\" width=\"500\" align=\"center\"/>  \n","\n","- `MultiHeadedAttention`\n","- `PositionwiseFeedForward`"]},{"cell_type":"markdown","metadata":{"id":"7o1-iOBu99ss"},"source":["### Challenge\n"]},{"cell_type":"markdown","metadata":{"id":"V0ochH0n99st"},"source":["### Q1. 위 도식에 따라 `score`, `p_attn`, `attention` 을 구하라 "]},{"cell_type":"code","execution_count":11,"metadata":{"id":"ZMYcy8h499sv","executionInfo":{"status":"ok","timestamp":1646904233789,"user_tz":-540,"elapsed":3,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["def attention(query, key, value, mask=None, dropout=None):\n","    d_k = query.size(-1)\n","    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n","             / math.sqrt(d_k)\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","    p_attn = F.softmax(scores, dim = -1)\n","    if dropout is not None:\n","        p_attn = dropout(p_attn)\n","    return torch.matmul(p_attn, value), p_attn"]},{"cell_type":"markdown","metadata":{"id":"x25aeigL99sw"},"source":["###Q2. query, key, value가 모두 (m, d_k) shape의 matrix라고 가정할 때, `score`, `p_attn`, `attention`의 shape을 각각 구하라\n","- score : \n","- p_attn : \n","- attention : "]},{"cell_type":"markdown","metadata":{"id":"IHfxLJKz99sx"},"source":["### (아래의 **Q3을 먼저 풀고 돌아오세요**) Q4.  query, key, value가 모두 (12, 8, 1, 64) shape의 tensor라고 가정할 때 , `score`, `p_attn`, `attention`의 shape을 각각 구하라\n","\n","- score : \n","- p_attn : \n","- attention : "]},{"cell_type":"markdown","metadata":{"id":"wYnffQE799sy"},"source":["- `MultiHeadedAttention`\n","\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_multihead.png?raw=true\" width=\"300\" align=\"center\"/>  "]},{"cell_type":"code","execution_count":12,"metadata":{"id":"uhFKlJ2b99sz","executionInfo":{"status":"ok","timestamp":1646904234964,"user_tz":-540,"elapsed":411,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["class MultiHeadedAttention(nn.Module):\n","    def __init__(self, h, d_model, dropout=0.1):\n","        super(MultiHeadedAttention, self).__init__()\n","        assert d_model % h == 0\n","        # We assume d_v always equals d_k\n","        self.d_k = d_model // h\n","        self.h = h\n","        self.linears = clones(nn.Linear(d_model, d_model), 4)\n","        self.attn = None\n","        self.dropout = nn.Dropout(p=dropout)\n","    \n","    \n","    def forward(self, query, key, value, mask=None):\n","        if mask is not None:\n","            # Same mask applied to all h heads.\n","            mask = mask.unsqueeze(1)\n","        nbatches = query.size(0)\n","        \n","        # 1) Do all the linear projections in batch from d_model => h x d_k \n","        query, key, value = \\\n","            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","             for l, x in zip(self.linears, (query, key, value))]\n","        \n","        # 2) Apply attention on all the projected vectors in batch. \n","        x, self.attn = attention(query, key, value, mask=mask, \n","                                 dropout=self.dropout)\n","        \n","        # 3) \"Concat\" using a view and apply a final linear. \n","        x = x.transpose(1, 2).contiguous() \\\n","             .view(nbatches, -1, self.h * self.d_k)\n","        return self.linears[-1](x)"]},{"cell_type":"markdown","metadata":{"id":"M46Ensa499s0"},"source":["### Q3.  query, key, value가 모두 (12, 512) shape의 matrix이고, h 값이 8 이라고 가정할 때, 아래 값의 shape을 각각 구하라\n","\n","- `d_k` (d_k = d_model // h) : \n","- `nn.Linear(d_model, d_model)(query)` : \n","- `nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, d_k)` : \n","- `nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, d_k).transpose(1,2)` : "]},{"cell_type":"markdown","metadata":{"id":"twZoeFr799s1"},"source":["- `PositionwiseFeedForward`\n","\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_pwff.png?raw=true\" width=\"300\" align=\"center\"/>  "]},{"cell_type":"code","execution_count":13,"metadata":{"id":"nZzpucvQ99s2","executionInfo":{"status":"ok","timestamp":1646904235975,"user_tz":-540,"elapsed":2,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["class PositionwiseFeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.w_1 = nn.Linear(d_model, d_ff)\n","        self.w_2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return self.w_2(self.dropout(F.relu(self.w_1(x))))"]},{"cell_type":"markdown","metadata":{"id":"TqjsUsbu99s3"},"source":["### Input Embedding & Encoding\n","- `Embeddings`\n","    - [pytorch official docs](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"FBVJFurO99s3","executionInfo":{"status":"ok","timestamp":1646904236207,"user_tz":-540,"elapsed":2,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["class Embeddings(nn.Module):\n","  \n","    def __init__(self, d_model, vocab):\n","        super(Embeddings, self).__init__()\n","        self.lut = nn.Embedding(vocab, d_model)\n","        self.d_model = d_model\n","    \n","    def forward(self, x):\n","        return self.lut(x) * math.sqrt(self.d_model)"]},{"cell_type":"markdown","metadata":{"id":"Po31qs_A99s5"},"source":["- `PositionalEncoding`\n","\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_pe.png?raw=true\" width=\"500\" align=\"center\"/>  \n","\n","- `position` 변수 설명\n","    - 모든 position (=최대 토큰 개수)의 값을 갖고 있는 matrix\n","- `div_term` 변수 설명\n","\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_div.png?raw=true\" width=\"500\" align=\"center\"/>  \n","- `Embedding` + `Encoding` 도식화 \n","\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_emb_enc.png?raw=true\" width=\"400\" align=\"center\"/>  \n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"RP-_an3x99s5","executionInfo":{"status":"ok","timestamp":1646904237189,"user_tz":-540,"elapsed":2,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    \n","    def __init__(self, d_model, dropout, max_len = 5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        \n","        # Compute the positional encodings once in log space.\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) *\n","                             -(math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","        \n","    def forward(self, x):\n","        x = x + Variable(self.pe[:, :x.size(1)], \n","                         requires_grad=False)\n","        return self.dropout(x)"]},{"cell_type":"markdown","metadata":{"id":"kNf13Gkm99s6"},"source":["### Q4.  max_len이 512이고, d_model이 512라고 가정할 때, `position`과 `div_term`의 shape을 구하라\n","\n","- `position` : \n","- `div_term` : \n","- `position * div_term` : "]},{"cell_type":"markdown","metadata":{"id":"Rri-daP399s7"},"source":["### Advanced"]},{"cell_type":"markdown","metadata":{"id":"N3ZixTN199s8"},"source":["### Finally Build Model\n","- Xavier Initialization\n","    - [한국어 자료](https://huangdi.tistory.com/8)\n","    - [pytorch official docs](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"kPdGsCiC99s8","executionInfo":{"status":"ok","timestamp":1646904239174,"user_tz":-540,"elapsed":280,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}}},"outputs":[],"source":["def make_model(src_vocab, tgt_vocab, \n","               N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n","    c = copy.deepcopy\n","    attn = MultiHeadedAttention(h, d_model)\n","    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n","    position = PositionalEncoding(d_model, dropout)\n","    model = EncoderDecoder(\n","        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n","        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n","                             c(ff), dropout), N),\n","        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n","        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n","        Generator(d_model, tgt_vocab))\n","    \n","    # This was important from their code. \n","    # Initialize parameters with Glorot / fan_avg.\n","    for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform(p)\n","    return model\n","            "]},{"cell_type":"code","execution_count":17,"metadata":{"id":"eIDN1DSd99s-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646904240398,"user_tz":-540,"elapsed":834,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}},"outputId":"6950f73b-10d5-46f3-9802-377235d35308"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"]}],"source":["model = make_model(10,10)"]},{"cell_type":"markdown","metadata":{"id":"ljRK80Lo99s_"},"source":["### Q5. 위 코드로 만든 모델의 모든 파라미터의 이름과 크기 (shape) 을 출력하라"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"BHubCUOh99tA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646904244006,"user_tz":-540,"elapsed":246,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}},"outputId":"a4206833-0d0d-4a39-8a93-3694cfdd24d1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["EncoderDecoder(\n","  (encoder): Encoder(\n","    (layers): ModuleList(\n","      (0): EncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): EncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): EncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): EncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): EncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): EncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNorm()\n","  )\n","  (decoder): Decoder(\n","    (layers): ModuleList(\n","      (0): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=512, out_features=512, bias=True)\n","            (1): Linear(in_features=512, out_features=512, bias=True)\n","            (2): Linear(in_features=512, out_features=512, bias=True)\n","            (3): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNorm()\n","  )\n","  (src_embed): Sequential(\n","    (0): Embeddings(\n","      (lut): Embedding(10, 512)\n","    )\n","    (1): PositionalEncoding(\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (tgt_embed): Sequential(\n","    (0): Embeddings(\n","      (lut): Embedding(10, 512)\n","    )\n","    (1): PositionalEncoding(\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (generator): Generator(\n","    (proj): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":18}],"source":["# 구현\n","model"]},{"cell_type":"code","source":["for name, param in model.named_parameters():\n","    print(\"--------------------\")\n","    print(\"Name: \", name)\n","    print(\"Shape: \", param.shape)\n","    print(\"--------------------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DYySXP63JHMk","executionInfo":{"status":"ok","timestamp":1646904591468,"user_tz":-540,"elapsed":254,"user":{"displayName":"태현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-B9t3vs0b_9jf_6RKFPgQs-SbQ9AszToZq0jvgw=s64","userId":"06295366345113326026"}},"outputId":"c05569de-4d40-4de9-ee91-164114c84503"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------\n","Name:  encoder.layers.0.self_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.0.self_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.0.self_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.0.self_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.0.self_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.0.self_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.0.self_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.0.self_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.0.feed_forward.w_1.weight\n","Shape:  torch.Size([2048, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.0.feed_forward.w_1.bias\n","Shape:  torch.Size([2048])\n","--------------------\n","--------------------\n","Name:  encoder.layers.0.feed_forward.w_2.weight\n","Shape:  torch.Size([512, 2048])\n","--------------------\n","--------------------\n","Name:  encoder.layers.0.feed_forward.w_2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.0.sublayer.0.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.0.sublayer.0.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.0.sublayer.1.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.0.sublayer.1.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.1.self_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.1.self_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.1.self_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.1.self_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.1.self_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.1.self_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.1.self_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.1.self_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.1.feed_forward.w_1.weight\n","Shape:  torch.Size([2048, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.1.feed_forward.w_1.bias\n","Shape:  torch.Size([2048])\n","--------------------\n","--------------------\n","Name:  encoder.layers.1.feed_forward.w_2.weight\n","Shape:  torch.Size([512, 2048])\n","--------------------\n","--------------------\n","Name:  encoder.layers.1.feed_forward.w_2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.1.sublayer.0.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.1.sublayer.0.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.1.sublayer.1.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.1.sublayer.1.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.2.self_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.2.self_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.2.self_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.2.self_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.2.self_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.2.self_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.2.self_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.2.self_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.2.feed_forward.w_1.weight\n","Shape:  torch.Size([2048, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.2.feed_forward.w_1.bias\n","Shape:  torch.Size([2048])\n","--------------------\n","--------------------\n","Name:  encoder.layers.2.feed_forward.w_2.weight\n","Shape:  torch.Size([512, 2048])\n","--------------------\n","--------------------\n","Name:  encoder.layers.2.feed_forward.w_2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.2.sublayer.0.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.2.sublayer.0.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.2.sublayer.1.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.2.sublayer.1.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.3.self_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.3.self_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.3.self_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.3.self_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.3.self_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.3.self_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.3.self_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.3.self_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.3.feed_forward.w_1.weight\n","Shape:  torch.Size([2048, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.3.feed_forward.w_1.bias\n","Shape:  torch.Size([2048])\n","--------------------\n","--------------------\n","Name:  encoder.layers.3.feed_forward.w_2.weight\n","Shape:  torch.Size([512, 2048])\n","--------------------\n","--------------------\n","Name:  encoder.layers.3.feed_forward.w_2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.3.sublayer.0.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.3.sublayer.0.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.3.sublayer.1.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.3.sublayer.1.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.4.self_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.4.self_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.4.self_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.4.self_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.4.self_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.4.self_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.4.self_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.4.self_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.4.feed_forward.w_1.weight\n","Shape:  torch.Size([2048, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.4.feed_forward.w_1.bias\n","Shape:  torch.Size([2048])\n","--------------------\n","--------------------\n","Name:  encoder.layers.4.feed_forward.w_2.weight\n","Shape:  torch.Size([512, 2048])\n","--------------------\n","--------------------\n","Name:  encoder.layers.4.feed_forward.w_2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.4.sublayer.0.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.4.sublayer.0.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.4.sublayer.1.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.4.sublayer.1.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.5.self_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.5.self_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.5.self_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.5.self_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.5.self_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.5.self_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.5.self_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.5.self_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.5.feed_forward.w_1.weight\n","Shape:  torch.Size([2048, 512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.5.feed_forward.w_1.bias\n","Shape:  torch.Size([2048])\n","--------------------\n","--------------------\n","Name:  encoder.layers.5.feed_forward.w_2.weight\n","Shape:  torch.Size([512, 2048])\n","--------------------\n","--------------------\n","Name:  encoder.layers.5.feed_forward.w_2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.5.sublayer.0.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.5.sublayer.0.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.5.sublayer.1.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.layers.5.sublayer.1.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  encoder.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.self_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.self_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.self_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.self_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.self_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.self_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.self_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.self_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.src_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.src_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.src_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.src_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.src_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.src_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.src_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.src_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.feed_forward.w_1.weight\n","Shape:  torch.Size([2048, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.feed_forward.w_1.bias\n","Shape:  torch.Size([2048])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.feed_forward.w_2.weight\n","Shape:  torch.Size([512, 2048])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.feed_forward.w_2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.sublayer.0.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.sublayer.0.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.sublayer.1.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.sublayer.1.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.sublayer.2.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.0.sublayer.2.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.self_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.self_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.self_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.self_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.self_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.self_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.self_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.self_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.src_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.src_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.src_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.src_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.src_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.src_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.src_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.src_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.feed_forward.w_1.weight\n","Shape:  torch.Size([2048, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.feed_forward.w_1.bias\n","Shape:  torch.Size([2048])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.feed_forward.w_2.weight\n","Shape:  torch.Size([512, 2048])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.feed_forward.w_2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.sublayer.0.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.sublayer.0.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.sublayer.1.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.sublayer.1.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.sublayer.2.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.1.sublayer.2.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.self_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.self_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.self_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.self_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.self_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.self_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.self_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.self_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.src_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.src_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.src_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.src_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.src_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.src_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.src_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.src_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.feed_forward.w_1.weight\n","Shape:  torch.Size([2048, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.feed_forward.w_1.bias\n","Shape:  torch.Size([2048])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.feed_forward.w_2.weight\n","Shape:  torch.Size([512, 2048])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.feed_forward.w_2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.sublayer.0.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.sublayer.0.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.sublayer.1.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.sublayer.1.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.sublayer.2.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.2.sublayer.2.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.self_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.self_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.self_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.self_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.self_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.self_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.self_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.self_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.src_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.src_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.src_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.src_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.src_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.src_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.src_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.src_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.feed_forward.w_1.weight\n","Shape:  torch.Size([2048, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.feed_forward.w_1.bias\n","Shape:  torch.Size([2048])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.feed_forward.w_2.weight\n","Shape:  torch.Size([512, 2048])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.feed_forward.w_2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.sublayer.0.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.sublayer.0.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.sublayer.1.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.sublayer.1.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.sublayer.2.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.3.sublayer.2.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.self_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.self_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.self_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.self_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.self_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.self_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.self_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.self_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.src_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.src_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.src_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.src_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.src_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.src_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.src_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.src_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.feed_forward.w_1.weight\n","Shape:  torch.Size([2048, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.feed_forward.w_1.bias\n","Shape:  torch.Size([2048])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.feed_forward.w_2.weight\n","Shape:  torch.Size([512, 2048])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.feed_forward.w_2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.sublayer.0.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.sublayer.0.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.sublayer.1.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.sublayer.1.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.sublayer.2.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.4.sublayer.2.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.self_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.self_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.self_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.self_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.self_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.self_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.self_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.self_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.src_attn.linears.0.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.src_attn.linears.0.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.src_attn.linears.1.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.src_attn.linears.1.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.src_attn.linears.2.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.src_attn.linears.2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.src_attn.linears.3.weight\n","Shape:  torch.Size([512, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.src_attn.linears.3.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.feed_forward.w_1.weight\n","Shape:  torch.Size([2048, 512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.feed_forward.w_1.bias\n","Shape:  torch.Size([2048])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.feed_forward.w_2.weight\n","Shape:  torch.Size([512, 2048])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.feed_forward.w_2.bias\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.sublayer.0.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.sublayer.0.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.sublayer.1.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.sublayer.1.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.sublayer.2.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.layers.5.sublayer.2.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.norm.a_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  decoder.norm.b_2\n","Shape:  torch.Size([512])\n","--------------------\n","--------------------\n","Name:  src_embed.0.lut.weight\n","Shape:  torch.Size([10, 512])\n","--------------------\n","--------------------\n","Name:  tgt_embed.0.lut.weight\n","Shape:  torch.Size([10, 512])\n","--------------------\n","--------------------\n","Name:  generator.proj.weight\n","Shape:  torch.Size([10, 512])\n","--------------------\n","--------------------\n","Name:  generator.proj.bias\n","Shape:  torch.Size([10])\n","--------------------\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"9yOuSvefJZYE"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Week3_4_assginment.ipynb","provenance":[{"file_id":"1FO1d5-UYxY5ldxx61bdRmwPjPX4bT5Au","timestamp":1646899753590}]},"kernelspec":{"display_name":"torch","language":"python","name":"torch"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"}},"nbformat":4,"nbformat_minor":0}